{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from utils_laj import *\n",
    "from data_processing import get_SensorData, my_pca, series_to_supervised, lstm_sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "today = datetime.date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = './save/save_lstm/lstm_2_layers'\n",
    "input_path = './input_data'\n",
    "file = os.path.join(input_path,'sensors.csv')\n",
    "target_file = os.path.join(input_path,'coke_target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считываем данные из файлов, нормализуем и применяем метод главных комполнентов\n",
    "X, y, submit_X, mean_y, std_y, pca = get_SensorData(file, target_file, nc=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66839234 0.05001844 0.0454892  0.04043455 0.02760575 0.02430107\n",
      " 0.02172642 0.01984493 0.01803035 0.01471829] [0.66839234 0.71841079 0.76389998 0.80433454 0.83194029 0.85624136\n",
      " 0.87796778 0.89781272 0.91584307 0.93056136]\n"
     ]
    }
   ],
   "source": [
    "evals = pca.explained_variance_ratio_\n",
    "evals_cs = evals.cumsum()\n",
    "print(evals, evals_cs)\n",
    "# Кумулятивное variance_explained равно 93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data X: (10400, 11), y (10400, 2), submit_X (13272, 11)\n"
     ]
    }
   ],
   "source": [
    "# Размерности датасета с известным целевым y и полного датасета\n",
    "print(\"Data X: %s, y %s, submit_X %s\" % (X.shape, y.shape, submit_X.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# преобразование датасета для возможности supervised learning\n",
    "# используем для обучения и валидации модели данные за предыдущие 48 часов (time_length)\n",
    "time_length = 48\n",
    "Xl = series_to_supervised(X.iloc[:,1:len(list(X))], n_in=time_length, n_out=1)\n",
    "yl = y['target'][time_length:]\n",
    "print(\"Data Xl: %s, yl %s\" % (Xl.shape, yl.shape))\n",
    "# формируем данные конкурса по аналогичному принципу\n",
    "Xs = series_to_supervised(submit_X.iloc[(Xl.shape[0]-time_length):,1:len(list(submit_X))], n_in=time_length, n_out=1)\n",
    "\n",
    "# выделим последние 25% в тестовый датасет\n",
    "x_train, x_test, y_train, y_test = train_test_split(Xl, yl, test_size=0.25, random_state=42, shuffle=False)\n",
    "print(\"Data x_train: %s, x_test: %s, y_train %s, y_test\" % (X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples length 10352\n",
      "Targets length 10352\n",
      "Data Xl: (10352, 48, 10), yl (10352, 48)\n",
      "Samples length 2872\n",
      "Targets length 0\n",
      "Data X_train: (7764, 48, 10), X_test: (2588, 48, 10), y_train (7764, 48), y_test (2588, 48), Xs (2872, 48, 10)\n"
     ]
    }
   ],
   "source": [
    "# Формируем сэмплы для LSTM по 250 рядов внахлест с шагом 1\n",
    "timesteps = 48\n",
    "Xl, yl = lstm_sampling(X.iloc[:,1:len(list(X))], y=y['target'], timesteps=timesteps)\n",
    "#yl = y['target'][timesteps:]\n",
    "print(\"Data Xl: %s, yl %s\" % (Xl.shape, yl.shape))\n",
    "# формируем данные конкурса по аналогичному принципу\n",
    "Xs, ys = lstm_sampling(submit_X.iloc[(Xl.shape[0]):,1:len(list(submit_X))], timesteps=timesteps)\n",
    "\n",
    "# выделим последние 25% в тестовый датасет\n",
    "x_train, x_test, y_train, y_test = train_test_split(Xl, yl, test_size=0.25, random_state=42, shuffle=False)\n",
    "print(\"Data X_train: %s, X_test: %s, y_train %s, y_test %s, Xs %s\" % (x_train.shape, x_test.shape, y_train.shape, y_test.shape, Xs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Train = True\n",
    "    Predict = True\n",
    "    plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    batch_size = 1024  # Batch size\n",
    "    if Train == False: batch_size = 1\n",
    "\n",
    "    sequence_length = timesteps  # Number of steps\n",
    "    learning_rate = 5*10e-5  # 0.0001\n",
    "    epochs = 1000\n",
    "    ann_hidden = 8\n",
    "\n",
    "    n_channels = x_train.shape[2]\n",
    "\n",
    "    lstm_size = 24  # Number LSTM units\n",
    "    num_layers = 2  # 2  # Number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Conv Shape: [None, 48, 10]\n"
     ]
    }
   ],
   "source": [
    "    X = tf.placeholder(tf.float32, [None, sequence_length, n_channels], name='inputs')\n",
    "    Y = tf.placeholder(tf.float32, [None, sequence_length], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    is_train = tf.placeholder(dtype=tf.bool, shape=None, name=\"is_train\")\n",
    "\n",
    "    conv_last_layer = X\n",
    "\n",
    "    shape = conv_last_layer.get_shape().as_list()\n",
    "    print('My Conv Shape:',shape)\n",
    "    CNN_flat = tf.reshape(conv_last_layer, [-1, shape[1] * shape[2]])\n",
    "\n",
    "    dence_layer_1 = dense_layer(CNN_flat, size=sequence_length * n_channels, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "                                scope=\"fc_1\")\n",
    "    lstm_input = tf.reshape(dence_layer_1, [-1, sequence_length, n_channels])\n",
    "\n",
    "    cell = get_RNNCell(['LSTM'] * num_layers, keep_prob=keep_prob, state_size=lstm_size)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_output, states = tf.nn.dynamic_rnn(cell, lstm_input, dtype=tf.float32, initial_state=init_state)\n",
    "    stacked_rnn_output = tf.reshape(rnn_output, [-1, lstm_size])  # change the form into a tensor\n",
    "\n",
    "    dence_layer_2 = dense_layer(stacked_rnn_output, size=ann_hidden, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "                                scope=\"fc_2\")\n",
    "    \n",
    "    #dence_layer_3 = dense_layer(dence_layer_2, size=ann_hidden, activation_fn=tf.nn.relu, batch_norm=False,\n",
    "    #                            phase=is_train, drop_out=True, keep_prob=keep_prob,\n",
    "    #                            scope=\"fc_2_2\")\n",
    "\n",
    "    output = dense_layer(dence_layer_2, size=1, activation_fn=None, batch_norm=False, phase=is_train, drop_out=False,\n",
    "                         keep_prob=keep_prob,\n",
    "                         scope=\"fc_3_output\")\n",
    "\n",
    "    prediction = tf.reshape(output, [-1])\n",
    "    y_flat = tf.reshape(Y, [-1])\n",
    "\n",
    "    h = prediction - y_flat\n",
    "\n",
    "    cost_function = tf.reduce_sum(tf.square(h))\n",
    "    RMSE = tf.sqrt(tf.reduce_mean(tf.square(h)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost_function)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    training_generator = batch_generator(x_train, y_train, batch_size, sequence_length, online=True)\n",
    "    testing_generator = batch_generator(x_test, y_test, batch_size, sequence_length, online=False)\n",
    "\n",
    "    if Train: model_summary(learning_rate=learning_rate, batch_size=batch_size, lstm_layers=num_layers,\n",
    "                            lstm_layer_size=lstm_size, fc_layer_size=ann_hidden, sequence_length=sequence_length,\n",
    "                            n_channels=n_channels, path_checkpoint=path_checkpoint, spacial_note='')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set MSE\n",
      "No epoches:  1000 No itr:  7764\n"
     ]
    }
   ],
   "source": [
    "    with tf.Session() as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        if Train == True:\n",
    "            #saver.restore(session, path_checkpoint)\n",
    "            #print(\"Model restored from file: %s\" % path_checkpoint)\n",
    "\n",
    "            cost = []\n",
    "            iteration = int(x_train.shape[0])\n",
    "            print(\"Training set MSE\")\n",
    "            print(\"No epoches: \", epochs, \"No itr: \", iteration)\n",
    "            __start = time.time()\n",
    "            for ep in range(epochs):\n",
    "                for itr in range(iteration):\n",
    "                    ## training ##\n",
    "                    batch_x, batch_y = next(training_generator)\n",
    "                    session.run(optimizer,\n",
    "                                feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.7, learning_rate_: learning_rate})\n",
    "                    cost.append(\n",
    "                        RMSE.eval(feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.7, learning_rate_: learning_rate}))\n",
    "\n",
    "                x_test_batch, y_test_batch = next(testing_generator)\n",
    "                mse_train, rmse_train = session.run([cost_function, RMSE],\n",
    "                                                    feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0,\n",
    "                                                               learning_rate_: learning_rate})\n",
    "                mse_test, rmse_test = session.run([cost_function, RMSE],\n",
    "                                                  feed_dict={X: x_test_batch, Y: y_test_batch, keep_prob: 1.0,\n",
    "                                                             learning_rate_: learning_rate})\n",
    "\n",
    "                time_per_ep = (time.time() - __start)\n",
    "                time_remaining = ((epochs - ep) * time_per_ep) / 3600\n",
    "                print(\"CNNLSTM\", \"epoch:\", ep, \"\\tTrainig-\",\n",
    "                      \"MSE:\", mse_train, \"RMSE:\", rmse_train, \"\\tTesting-\", \"MSE\", mse_test, \"RMSE\", rmse_test,\n",
    "                      \"\\ttime/epoch:\", round(time_per_ep, 2), \"\\ttime_remaining: \",\n",
    "                      int(time_remaining), \" hr:\", round((time_remaining % 1) * 60, 1), \" min\", \"\\ttime_stamp: \",\n",
    "                      datetime.datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\"))\n",
    "                __start = time.time()\n",
    "\n",
    "                if ep % 10 == 0 and ep != 0:\n",
    "                    save_path = saver.save(session, path_checkpoint)\n",
    "                    if os.path.exists(path_checkpoint + '.meta'):\n",
    "                        print(\"Model saved to file: %s\" % path_checkpoint)\n",
    "                    else:\n",
    "                        print(\"NOT SAVED!!!\", path_checkpoint)\n",
    "\n",
    "                if ep % 1000 == 0 and ep != 0: learning_rate = learning_rate / 10\n",
    "\n",
    "            save_path = saver.save(session, path_checkpoint)\n",
    "            if os.path.exists(path_checkpoint + '.meta'):\n",
    "                print(\"Model saved to file: %s\" % path_checkpoint)\n",
    "            else:\n",
    "                print(\"NOT SAVED!!!\", path_checkpoint)\n",
    "            plt.plot(cost)\n",
    "            plt.show()\n",
    "        else:\n",
    "            saver.restore(session, path_checkpoint)\n",
    "            print(\"Model restored from file: %s\" % path_checkpoint)\n",
    "            if Predict == True:\n",
    "                print(\"Prediction for submit...\")\n",
    "                x_predict = Xs\n",
    "                y_predict = np.zeros((Xs.shape[0],Xs.shape[1]))\n",
    "\n",
    "                predict_generator = batch_generator(x_predict, y_predict, batch_size, sequence_length,\n",
    "                                                       online=True)\n",
    "\n",
    "                full_prediction = []\n",
    "\n",
    "                iteration = int(x_predict.shape[0] / (batch_size ))\n",
    "                print(\"#of validation points:\", x_predict.shape[0], \"#datapoints covers from minibatch:\",\n",
    "                      batch_size * sequence_length, \"iterations/epoch\", iteration)\n",
    "\n",
    "                for itr in range(iteration):\n",
    "                    x_validate_batch, y_validate_batch = next(predict_generator)\n",
    "                    __y_pred, error, __y = session.run([prediction, h, y_flat],\n",
    "                                                       feed_dict={X: x_validate_batch, Y: y_validate_batch,\n",
    "                                                                  keep_prob: 1.0})\n",
    "                    full_prediction.append(__y_pred)\n",
    "                    print(__y_pred)\n",
    "                    \n",
    "                full_prediction = np.array(full_prediction)\n",
    "                full_prediction = full_prediction.ravel()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit = my_gbm.predict(Xs)*std_y+mean_y\n",
    "sub_file = pd.DataFrame()\n",
    "sub_file[\"timestamp\"] = submit_X[\"timestamp\"][Xl.shape[0]+time_length:]\n",
    "sub_file[\"target\"] = y_submit\n",
    "sub_file.to_csv('submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
